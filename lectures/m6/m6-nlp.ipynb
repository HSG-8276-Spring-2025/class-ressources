{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec89b0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://raw.githubusercontent.com/malkaguillot/Foundations-in-Data-Science-and-Machine-Learning/refs/heads/main/docs/utils/custom.css\">\n",
       "%%HTML\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"../utils/custom.css\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://raw.githubusercontent.com/malkaguillot/Foundations-in-Data-Science-and-Machine-Learning/refs/heads/main/docs/utils/custom.css\">\n",
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../utils/custom.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f702504e",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Foundations in Data Science and Machine Learning\n",
    "# Module 6: Natural Language Processing\n",
    "### [Malka Guillot](https://malkaguillot.github.io/)\n",
    "\n",
    "<img src=\"../utils/img/logo-hsg.png\" alt=\"HSG Logo\" style=\"position: relative; bottom: 50px; left: 600px; width: 100px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7f219f",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Table of Contents\n",
    "1. Prologue\n",
    "2. Dictionary methods\n",
    "3. Tokenization\n",
    "4. Measures of document distances\n",
    "4. Topic models\n",
    "5. Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45a0f98",
   "metadata": {},
   "source": [
    "## References   \n",
    "-  Ludwig, J., Mullainathan, S., & Rambachan, A. (2025). Large language models: An applied econometric framework (No. w33344). National Bureau of Economic Research. [link](https://www.nber.org/papers/w33344)\n",
    "\n",
    "-  Ash, E., & Hansen, S. (2023). Text algorithms in economics. Annual Review of Economics, 15(1), 659-688. [link](https://www.annualreviews.org/content/journals/10.1146/annurev-economics-082222-074352)\n",
    "   -  [Notebooks](https://github.com/sekhansen/text_algorithms_econ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629ccb5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prologue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf1939",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Motivation\n",
    "\n",
    "- Much of economic research has been using structured data\n",
    " - Usually stored in a relational database\n",
    " - Sometimes called relational data\n",
    " - Can be easily mapped into specific fields\n",
    "\n",
    "- Research involving unstructured data is on the rise\n",
    "  - Text, images/videos, audio recordings, ... → treasures for (social science) researchers\n",
    "  - Has long required analysis by humans\n",
    "  - With machine learning and AI, tools to work with vast quantities of such data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a80ff52",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [Motivation] The rise of text data\n",
    "- This trend is in large part due to the digitization of our societies.\n",
    "\n",
    "- The digital era generates considerable amounts of text.\n",
    "  - Social media and internet queries\n",
    "  - Wikipedia, online newspapers, TV transcripts\n",
    "  - Digitized books, speeches, laws\n",
    "- It is matched with a similar increase in computational resources.\n",
    "  - Moore’s law = processing power of computers doubles every two years (since the 70s!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab8bf4",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [Motivation] Moore’s law\n",
    "= Processing power of computers doubles every two years (since the 70s!)\n",
    "<img src=\"images/moore-law.png\"  style=\"height: 350px; position:relative;     margin-left: auto;margin-right: auto;display: block\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef7402",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Natural language processing\n",
    "\n",
    "- Natural language processing is a data-driven approach to the analysis of text documents.\n",
    "\n",
    "- Applications in your everyday life:\n",
    "  - Search engines, translation services, spam detection\n",
    "- Applications in social science:\n",
    "  - Measuring economic policy uncertainty, news sentiment, racial and misogynistic bias, political and economic narratives, speech polarization\n",
    "  - Predicting protests, GDP growth, financial market fluctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eb4a51",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### This course\n",
    "- Focus on natural language processing in **applied economic research**\n",
    "  \n",
    "- Contents:\n",
    "  - Dictionary-based methods, measures of text distance, topic models, embeddings, supervised learning\n",
    "- Why is this useful for economic research?\n",
    "  - Measure economic/political/social concepts in texts\n",
    "    - New variables\n",
    "    - “Old” variables in new ways (e.g., more easily/flexibly)\n",
    "  - Use text-based variables as regressors or outcomes\n",
    "  - Assess the real-world impacts of language on government and the economy.\n",
    "  - In particular: new avenues to assess the relationship between the economy/politics and language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb3d4f6",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A special characteristic of text data: high dimensionality\n",
    "\n",
    "- Text is very high-dimensional\n",
    "\n",
    "- Sample of documents, each $n_L$ words long, drawn from vocabulary of $n_V$ words.\n",
    "- The unique representation of each document has dimension $n_V^{n_L}$ .\n",
    "  - For example: take a sample of 30-word Twitter messages using only the one thousand most common words in the English language\n",
    "    - $\\rightarrow$ Dimensionality $= 1000^{30} = 10^{32}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2e1595",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### “Text as Data” by Gentzkow, Kelly, Taddy (2017)\n",
    "\n",
    "Summarize the analysis in three steps:\n",
    "\n",
    "1. Convert raw text $D$ to numerical array $\\mathbf{C}$\n",
    "  + The elements of $\\mathbf{C}$ are counts over tokens (words or phrases)\n",
    "\n",
    "2. Map $\\mathbf{C}$ to predicted values $\\mathbf{\\hat V}$ of unknown outcomes $\\mathbf{V}$\n",
    "  + Learn $\\mathbf{\\hat V(C)}$ using machine learning\n",
    "  - Supervised learning: for some labeled $C_i$ and $V_i$\n",
    "  - Unsupervised learning: topics/dimensions just from $\\mathbf{C}$\n",
    "\n",
    "3. Use $\\hat V$ for subsequent descriptive or causal analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2adf80",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Corpora\n",
    "<div class=\"container\">\n",
    "  <!-- First oval shape -->\n",
    "  <div class=\"oval\">\n",
    "    Raw Data\n",
    "  </div>\n",
    "\n",
    "  <!-- First arrow -->\n",
    "  <div class=\"arrow\"></div>\n",
    "\n",
    "  <!-- Rectangular shape in the middle -->\n",
    "  <div class=\"rectangle\">\n",
    "    Corpus Collection & Preparation\n",
    "  </div>\n",
    "\n",
    "  <!-- Second arrow -->\n",
    "  <div class=\"arrow\"></div>\n",
    "\n",
    "  <!-- Second oval shape -->\n",
    "  <div class=\"oval\">\n",
    "    D<br>\n",
    "    Plain Text<br> Documents\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<style>\n",
    "  .container {\n",
    "    display: flex;\n",
    "    align-items: center;\n",
    "    justify-content: space-around; /* Reduces space around items */    margin-top: 20px;\n",
    "  }\n",
    "\n",
    "  .oval {\n",
    "    text-align: center;\n",
    "    line-height: 20px;\n",
    "    width: 150px;\n",
    "    height: 100px;\n",
    "    display: flex;\n",
    "    align-items: center;\n",
    "    justify-content: center;\n",
    "    font-family: Arial, sans-serif;\n",
    "    font-size: 18px;\n",
    "  }\n",
    "\n",
    "  .rectangle {\n",
    "    text-align: center;\n",
    "    line-height: 100px;\n",
    "    width: 250px;\n",
    "    height: 100px;\n",
    "    display: flex;\n",
    "    align-items: center;\n",
    "    justify-content: center;\n",
    "    font-family: Arial, sans-serif;\n",
    "    font-size: 16px;\n",
    "  }\n",
    "\n",
    "  .oval {\n",
    "    background-color: lightgray;\n",
    "    border-radius: 50%;\n",
    "    border: 1px solid black;\n",
    "  }\n",
    "\n",
    "  .rectangle {\n",
    "    background-color: #ffebcc;\n",
    "    border: 1px solid black;\n",
    "  }\n",
    "\n",
    "  .arrow {\n",
    "    display: flex;\n",
    "    align-items: center;\n",
    "    justify-content: center;\n",
    "    width: 50px;\n",
    "  }\n",
    "\n",
    "  .arrow::before {\n",
    "    content: \"→\";\n",
    "    font-size: 28px;\n",
    "  }\n",
    "\n",
    "  .bold {\n",
    "    font-style: italic;\n",
    "    font-weight: bold;\n",
    "  }\n",
    "</style>\n",
    "\n",
    "\n",
    "\n",
    "- Text data is a sequence of characters called <bcolor>documents</bcolor>\n",
    "  \n",
    "- The set of documents is the <bcolor> corpus</bcolor>, which we will call $D$\n",
    "- Text data is unstructured:\n",
    "  + Relevant/needed information mixed with (lots of) irrelevant unneeded information\n",
    "- All text data approaches throw away some information:\n",
    "  - Challenge: retaining valuable information\n",
    "- Tokenization and dimension reduction:\n",
    "  - Transforming an unstructured corpus $D$ to a usable matrix $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a124511e",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What counts as a document?\n",
    "\n",
    "The unit of analysis (the “document”) varies depending on the application:\n",
    "- Needs to be fine enough to fit the relevant metadata variation\n",
    "- More often than not, we care about metadata!\n",
    "- Should not be finer than necessary – to avoid high-dimensionality without relevant empirical variation \n",
    "\n",
    ">What should we use as the document here?\n",
    "1. Predicting whether a judge is right-wing or left-wing in partisan ideology, from their written opinions\n",
    "2. Predicting whether parliamentary speeches become more emotive in the run-up to an election"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a297d59d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1b0ff2c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    " # Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib notebook\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings = lambda *a, **kw: None\n",
    "\n",
    "import sklearn\n",
    "# to make this notebook's output identical at every run\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07d0bfb",
   "metadata": {},
   "source": [
    "## 20 Newsgroups dataset from `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e4acae",
   "metadata": {},
   "source": [
    "We use as an example the [**20 Newsgroups**](http://qwone.com/~jason/20Newsgroups/) dataset (from `sklearn`), a collection of about 20,000 newsgroup (message forum) documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05960162",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups() # object is a dictionary\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b014fea9",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for t\n"
     ]
    }
   ],
   "source": [
    "# Dataset description\n",
    "print(data['DESCR'][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c3176e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W, y = data['data'], data['target']\n",
    "n_samples = y.shape[0]\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a3ad0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`y` : news story categories\n",
    "`W` : a set of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cb4f5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10] # news story categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48555a3d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### One document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4512721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be \""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = W[0] # first document (news story)\n",
    "doc[:300] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e704d64",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Store the data in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "359a40df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "topic",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "f841440d-e548-4c42-9c9e-5166ea878add",
       "rows": [
        [
         "0",
         "From: lerxst@wam.umd.edu (where's my thing)\nSubject: WHAT car is this!?\nNntp-Posting-Host: rac3.wam.umd.edu\nOrganization: University of Maryland, College Park\nLines: 15\n\n I was wondering if anyone out there could enlighten me on this car I saw\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\nthe front bumper was separate from the rest of the body. This is \nall I know. If anyone can tellme a model name, engine specs, years\nof production, where this car is made, history, or whatever info you\nhave on this funky looking car, please e-mail.\n\nThanks,\n- IL\n   ---- brought to you by your neighborhood Lerxst ----\n\n\n\n\n",
         "7"
        ],
        [
         "1",
         "From: guykuo@carson.u.washington.edu (Guy Kuo)\nSubject: SI Clock Poll - Final Call\nSummary: Final call for SI clock reports\nKeywords: SI,acceleration,clock,upgrade\nArticle-I.D.: shelley.1qvfo9INNc3s\nOrganization: University of Washington\nLines: 11\nNNTP-Posting-Host: carson.u.washington.edu\n\nA fair number of brave souls who upgraded their SI clock oscillator have\nshared their experiences for this poll. Please send a brief message detailing\nyour experiences with the procedure. Top speed attained, CPU rated speed,\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\nfunctionality with 800 and 1.4 m floppies are especially requested.\n\nI will be summarizing in the next two days, so please add to the network\nknowledge base if you have done the clock upgrade and haven't answered this\npoll. Thanks.\n\nGuy Kuo <guykuo@u.washington.edu>\n",
         "4"
        ],
        [
         "2",
         "From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\nSubject: PB questions...\nOrganization: Purdue University Engineering Computer Network\nDistribution: usa\nLines: 36\n\nwell folks, my mac plus finally gave up the ghost this weekend after\nstarting life as a 512k way back in 1985.  sooo, i'm in the market for a\nnew machine a bit sooner than i intended to be...\n\ni'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\nof questions that (hopefully) somebody can answer:\n\n* does anybody know any dirt on when the next round of powerbook\nintroductions are expected?  i'd heard the 185c was supposed to make an\nappearence \"this summer\" but haven't heard anymore on it - and since i\ndon't have access to macleak, i was wondering if anybody out there had\nmore info...\n\n* has anybody heard rumors about price drops to the powerbook line like the\nones the duo's just went through recently?\n\n* what's the impression of the display on the 180?  i could probably swing\na 180 if i got the 80Mb disk rather than the 120, but i don't really have\na feel for how much \"better\" the display is (yea, it looks great in the\nstore, but is that all \"wow\" or is it really that good?).  could i solicit\nsome opinions of people who use the 160 and 180 day-to-day on if its worth\ntaking the disk size and money hit to get the active display?  (i realize\nthis is a real subjective question, but i've only played around with the\nmachines in a computer store breifly and figured the opinions of somebody\nwho actually uses the machine daily might prove helpful).\n\n* how well does hellcats perform?  ;)\n\nthanks a bunch in advance for any info - if you could email, i'll post a\nsummary (news reading time is at a premium with finals just around the\ncorner... :( )\n--\nTom Willis  \\  twillis@ecn.purdue.edu    \\    Purdue Electrical Engineering\n---------------------------------------------------------------------------\n\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\nNietzsche\n",
         "4"
        ],
        [
         "3",
         "From: jgreen@amber (Joe Green)\nSubject: Re: Weitek P9000 ?\nOrganization: Harris Computer Systems Division\nLines: 14\nDistribution: world\nNNTP-Posting-Host: amber.ssd.csd.harris.com\nX-Newsreader: TIN [version 1.1 PL9]\n\nRobert J.C. Kyanko (rob@rjck.UUCP) wrote:\n> abraxis@iastate.edu writes in article <abraxis.734340159@class1.iastate.edu>:\n> > Anyone know about the Weitek P9000 graphics chip?\n> As far as the low-level stuff goes, it looks pretty nice.  It's got this\n> quadrilateral fill command that requires just the four points.\n\nDo you have Weitek's address/phone number?  I'd like to get some information\nabout this chip.\n\n--\nJoe Green\t\t\t\tHarris Corporation\njgreen@csd.harris.com\t\t\tComputer Systems Division\n\"The only thing that really scares me is a person with no sense of humor.\"\n\t\t\t\t\t\t-- Jonathan Winters\n",
         "1"
        ],
        [
         "4",
         "From: jcm@head-cfa.harvard.edu (Jonathan McDowell)\nSubject: Re: Shuttle Launch Question\nOrganization: Smithsonian Astrophysical Observatory, Cambridge, MA,  USA\nDistribution: sci\nLines: 23\n\nFrom article <C5owCB.n3p@world.std.com>, by tombaker@world.std.com (Tom A Baker):\n>>In article <C5JLwx.4H9.1@cs.cmu.edu>, ETRAT@ttacs1.ttu.edu (Pack Rat) writes...\n>>>\"Clear caution & warning memory.  Verify no unexpected\n>>>errors. ...\".  I am wondering what an \"expected error\" might\n>>>be.  Sorry if this is a really dumb question, but\n> \n> Parity errors in memory or previously known conditions that were waivered.\n>    \"Yes that is an error, but we already knew about it\"\n> I'd be curious as to what the real meaning of the quote is.\n> \n> tom\n\n\nMy understanding is that the 'expected errors' are basically\nknown bugs in the warning system software - things are checked\nthat don't have the right values in yet because they aren't\nset till after launch, and suchlike. Rather than fix the code\nand possibly introduce new bugs, they just tell the crew\n'ok, if you see a warning no. 213 before liftoff, ignore it'.\n\n - Jonathan\n\n\n",
         "14"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  topic\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...      7\n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...      4\n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...      4\n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...      1\n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...     14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(W,columns=['text'])\n",
    "df['topic'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a7efcc",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- .slide:  id=\"dictionary\" class: left, inverse -->\n",
    "## Dictionary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d49787",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dictionary methods\n",
    "\n",
    "- Dictionary-based text methods \n",
    "  - use a pre-selected list of words or phrases to analyze a corpus.\n",
    "  - Use regular expressions for this task\n",
    " \n",
    "- Corpus-specific: counting sets of words or phrases across documents\n",
    "  - (e.g., number of times a judge says “justice” vs. “efficiency”)\n",
    "- General dictionaries: WordNet, LIWC, MFD, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e51fc",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: dictionary methods\n",
    "#### Baker, Bloom, and Davis (QJE 2016), “Measuring Policy Uncertainty”\n",
    "\n",
    "<div class=\"columns\">\n",
    "  <div class=\"column\">\n",
    "\n",
    "- For each newspaper on each day since 1985, tag each article mentioning:\n",
    "  1. Uncertainty word\n",
    "  2. Economy word\n",
    "  3. Policy word (eg “legislation”, “regulation”)\n",
    "   \n",
    "- Then, normalize the resulting article counts by the total newspaper articles that month\n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "<img src=\"images/baker-bloom-davis-qje.png\"  style=\"height: 250px; position:relative;     margin-left: auto;margin-right: auto;display: block\" >\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca49fac2",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### WordNet\n",
    "\n",
    "- English word database: 118K nouns, 12K verbs, 22K adjectives, 5K adverbs\n",
    "- Synonym sets (synsets) are a group of near-synonyms, plus a gloss (definition)\n",
    "  - Also contains information on antonyms (opposites), holonyms/meronyms (part-whole)\n",
    "- Nouns are organized in a categorical hierarchy (hence “WordNet”)\n",
    "  - “hypernym” – the higher category that a word is a member of\n",
    "  - “hyponyms” – members of the category identified by a word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408d514",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### General dictionaries\n",
    "\n",
    "- Function words (e.g. for, rather, than)\n",
    "  - Also called stopwords (often removed)\n",
    "  - Can be used to get at non-topical dimensions and identify authors\n",
    "\n",
    "- LIWC (pronounced “Luke”): Linguistic Inquiry and Word Counts\n",
    "  - 2300 words\n",
    "  - 70 lists of category-relevant words, e.g. “emotion”, “cognition”, “work”, “family”, “positive”, “negative”, etc.\n",
    "\n",
    "- Mohammad and Turney (2011)\n",
    "  - 10,000 words coded along four emotional dimensions: joy–sadness, anger-fear, trust-disgust, anticipation-surprise\n",
    "\n",
    "- Warriner et al (2013)\n",
    "  - Code 14,000 words along three emotional dimensions: valence, arousal, dominance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cead75",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "- Extract a “tone” dimension – positive, negative, neutral\n",
    "- Dictionaries are extensively used for sentiment analysis:\n",
    "  - Let $(w_i , s_i )$ be pairs of words $w_i$ and their associated sentiment score  $s_i\\in [−1, 1]$. e.g., (“perfect”, 0.8), (“awful”, -0.9)\n",
    "  - The sentiment score for any phrase $j$ of $k$ tokens is a weighted average:\n",
    "\n",
    "$$ s_j = \\frac{1}{K}\\sum_{i=1}^ks_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee568f81",
   "metadata": {
    "slide_type": "subslide"
   },
   "source": [
    "\n",
    "- The standard approach is lexicon-based, but they fail easily: e.g., “good” versus “not good” versus “not very good”\n",
    "- The huggingface model hub has a number of transformer-based sentiment models\n",
    "- Off-the-shelf scores may be trained on specific and/or biased corpora\n",
    "  - For example, online data\n",
    "  - May not work for other data, e.g., parliamentary speeches, legal texts..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f0df1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using the `vaderSentimentIntensityAnalyzer` from `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea8ba1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/malka/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "import nltk\n",
    "\n",
    "# Download the lexicon\n",
    "nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07924afa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.585, 'pos': 0.415, 'compound': 0.75}\n"
     ]
    }
   ],
   "source": [
    "# Import the lexicon\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Create an instance of SentimentIntensityAnalyzer\n",
    "sent_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Example\n",
    "sentence = \"VADER is pretty good at identifying the underlying sentiment of a text!\"\n",
    "print(sent_analyzer.polarity_scores(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7099e3e",
   "metadata": {},
   "source": [
    "#### For the news document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f89497b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.012, 'neu': 0.916, 'pos': 0.072, 'compound': 0.807}\n"
     ]
    }
   ],
   "source": [
    "sent_analyzer = SentimentIntensityAnalyzer()\n",
    "polarity = sent_analyzer.polarity_scores(doc)\n",
    "print(polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c66645b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Applying the sentiment analysis to the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa2a44c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = df.sample(frac=.2) # sample 20% of the dataset\n",
    "\n",
    "# apply compound sentiment score to data-frame\n",
    "def get_sentiment(snippet):\n",
    "    return sent_analyzer.polarity_scores(snippet)['compound']\n",
    "\n",
    "dfs['sentiment'] = dfs['text'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d782258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CLINTON: AM Press Briefing by Dee Dee Myers -- 4.15.93\\nOrganization: Project GNU, Free Sof',\n",
       " ' Newsletter, Part 2/4\\nReply-To: david@stat.com (David Dodell)\\nDistribution: world\\nOrganiza',\n",
       " \"CLINTON: President's Remarks at Town Hall Meeting\\nOrganization: MIT Artificial Intelligenc\",\n",
       " 'Final Public Dragon Magazine Update (Last chance for public bids)\\nKeywords: Dragon Magazin',\n",
       " 'CLINTON: Background BRiefing in Vancouver 4.4.93\\nOrganization: Project GNU, Free Software ']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.sort_values('sentiment',inplace=True)\n",
    "[x[60:150] for x  in dfs[-5:]['text']] # print beginning of most positive documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f302d35b",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### NLP “bias” is statistical bias\n",
    "\n",
    "<div style=\"display: flex; align-items: center;\">\n",
    "  <div style=\"flex: 1; padding: 10px;\">\n",
    "    <!-- Content for the left column -->\n",
    "    <ul style=\"font-size: smaller;\">\n",
    "    <li style=\"margin-bottom: 10px;\">Sentiment scores that are trained on annotated datasets also learn from the correlated non-sentiment information</li>\n",
    "\n",
    "  <li>Supervised sentiment models are <strong>confounded</strong> by correlated language factors</li>\n",
    "</ul>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 10px;\">\n",
    "    <!-- Content for the right column -->\n",
    "    <img src=\"images/sentiment-confounder.png\" alt=\"NLP Bias\" style=\"width: 100%;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "- For example, a model trained on movie reviews may learn that “good” is positive and “bad” is negative\n",
    "  - But it may also learn that “good” is more likely to be used in reviews of comedies, and “bad” in reviews of horror movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae75a15",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### (We already had this problem)\n",
    "- *Supervised models* (classifiers, regressors) learn features that are correlated with the label being annotated\n",
    "\n",
    "- *Unsupervised models* (topic models, word embeddings) learn correlations between topics/contexts\n",
    "\n",
    "- **Dictionary methods**, while having other limitations, mitigate this problem\n",
    "  - The researcher intentionally “regularizes” out spurious confounders with the targeted language dimension\n",
    "  - Helps explain why economists often still use dictionary methods..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965cb7b",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77ae1d9",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "A major goal of tokenization is to produce features that are\n",
    "- Predictive in the learning task\n",
    "- Interpretable by human investigators\n",
    "- Tractable enough to be easy to work with\n",
    "- Two broad approaches:\n",
    "  1. Convert documents to vectors, usually frequency distributions over pre-processed $N-$ grams\n",
    "  2. Convert documents to sequences of tokens for inputs to sequential models (e.g., BERT, GPT, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af9d3e9",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A standard tokenization pipeline\n",
    "<img src=\"images/nlp-pipeline.png\"  style=\"height: 250px; position:relative;     margin-left: auto;margin-right: auto;display: block\">\n",
    "\n",
    "Source: 'Natural Language Processing with Python', Loper, Klein, and Bird, Chapter 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d746eab8",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The Processing Pipeline: \n",
    "- We open a URL and read its HTML content, \n",
    "- remove the markup and select a slice of characters; \n",
    "- this is then tokenized and optionally converted into an nltk.Text object; \n",
    "- we can also lowercase all the words and extract the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5399225c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example text for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c0870d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Marie Curie was the first woman to win a Nobel Prize, the first person to win a Nobel Prize twice, and the only person to win a Nobel Prize in 2 scientific fields. Her husband, Pierre Curie, was a co-winner of her first Nobel Prize, making them the first married couple to win the Nobel Prize and launching the Curie family legacy of 5 Nobel Prizes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a65f918",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. Pre-processing text\n",
    "\n",
    "- A key piece of the “art” of text analysis is deciding what data to throw out\n",
    "  - Uninformative data add noise and reduce statistical precision\n",
    "  - They are also computationally costly\n",
    "\n",
    "- Pre-processing choices can affect down-stream results, especially in unsupervised learning tasks (Denny and Spirling, 2018)\n",
    "  - Some features are more interpretable: “taxes are” / “are high” vs “taxes are high”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd74e60e",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Capitalization\n",
    "\n",
    "- Removing capitalization is a standard corpus normalization technique\n",
    "  - Usually, the capitalized/non-capitalized version of a word is equivalent – e.g. words showing up capitalized at beginning of a sentence\n",
    "  - Capitalization uninformative\n",
    "\n",
    "- For some tasks, capitalization is important\n",
    "  -  Required for sentence splitting, part-of-speech tagging, named entity recognition, syntactic/semantic parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9d4c44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'marie curie was the first woman to win a nobel prize, the first person to win a nobel prize twice, and the only person to win a nobel prize in 2 scientific fields. her husband, pierre curie, was a co-winner of her first nobel prize, making them the first married couple to win the nobel prize and launching the curie family legacy of 5 nobel prizes.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_lower = text.lower() # go to lower-case\n",
    "text_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde0d294",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Remove punctuation?\n",
    "\n",
    "Inclusion of punctuation depends on the task:\n",
    "\n",
    "- If one vectorizes the document as a bag of words or bag of N-grams, punctuation won’t be needed\n",
    "- Like capitalization, punctuation is needed for annotations (sentence splitting, parts of speech, syntax, roles, etc.) or for text generators\n",
    "\n",
    "#### Drop numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b69e1bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marie curie was the first woman to win a nobel prize the first person to win a nobel prize twice and the only person to win a nobel prize in 2 scientific fields her husband pierre curie was a cowinner of her first nobel prize making them the first married couple to win the nobel prize and launching the curie family legacy of 5 nobel prizes\n"
     ]
    }
   ],
   "source": [
    "# recipe for fast punctuation removal\n",
    "from string import punctuation\n",
    "punc_remover = str.maketrans('','',punctuation)\n",
    "text_nopunc = text_lower.translate(punc_remover)\n",
    "print(text_nopunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351f92b6",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Stemming/lemmatizing\n",
    "\n",
    "- Stemming: reducing words to their root form\n",
    "  - e.g., “running” → “run”, “better” → “good”\n",
    "  - Porter stemmer, Snowball stemmer, Lancaster stemmer\n",
    "\n",
    "- Lemmatizing: reducing words to their dictionary form\n",
    "  - e.g., “better” → “better”, “running” → “run”\n",
    "  - WordNet lemmatizer, spaCy lemmatizer\n",
    "\n",
    "<img src=\"images/stem-vs-lemm.png\"  style=\"height: 200px; position:relative;     margin-left: auto;margin-right: auto;display: block\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b350877",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tax', 'tax', 'tax', 'taxat']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english') # snowball stemmer, english\n",
    "# remake list of tokens, replace with stemmed versions\n",
    "tokens_stemmed = [stemmer.stem(t) for t in ['tax','taxes','taxed','taxation']]\n",
    "print(tokens_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "638ad0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autobahn\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('german') # snowball stemmer, german\n",
    "print(stemmer.stem(\"Autobahnen\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dbd521",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Lemmatization with `WordNetLemmatizer` from `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd7b4d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/malka/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ecde970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corporation', 'corporation', 'corporate']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "[wnl.lemmatize(c) for c in ['corporation', 'corporations', 'corporate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a681b7d1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Pre-processing function (homemade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28f26c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "translator = str.maketrans('','',punctuation)\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = set(stopwords.words('english'))\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def normalize_text(doc):\n",
    "    \"Input doc and return clean list of tokens\"\n",
    "    doc = doc.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    lower = doc.lower() # all lower case\n",
    "    nopunc = lower.translate(translator) # remove punctuation\n",
    "    words = nopunc.split() # split into tokens\n",
    "    nostop = [w for w in words if w not in stoplist] # remove stopwords\n",
    "    no_numbers = [w if not w.isdigit() else '#' for w in nostop] # normalize numbers\n",
    "    stemmed = [stemmer.stem(w) for w in no_numbers] # stem each word\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e9ad50",
   "metadata": {},
   "source": [
    "##### Applying the pre-processing function to the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d54cea58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [lerxstwamumdedu, where, thing, subject, car, ...\n",
       "1    [guykuocarsonuwashingtonedu, guy, kuo, subject...\n",
       "2    [twillisececnpurdueedu, thoma, e, willi, subje...\n",
       "3    [jgreenamb, joe, green, subject, weitek, p9000...\n",
       "4    [jcmheadcfaharvardedu, jonathan, mcdowel, subj...\n",
       "Name: tokens_cleaned, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens_cleaned'] = df['text'].apply(normalize_text)\n",
    "df['tokens_cleaned'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55632158",
   "metadata": {},
   "source": [
    "#### Pre-processing function (readymade)\n",
    "**Shortcut: `gensim.simple_preprocess`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3315662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bac49a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marie', 'curie', 'was', 'the', 'first', 'woman', 'to', 'win', 'nobel', 'prize', 'the', 'first', 'person', 'to', 'win', 'nobel', 'prize', 'twice', 'and', 'the', 'only', 'person', 'to', 'win', 'nobel', 'prize', 'in', 'scientific', 'fields', 'her', 'husband', 'pierre', 'curie', 'was', 'co', 'winner', 'of', 'her', 'first', 'nobel', 'prize', 'making', 'them', 'the', 'first', 'married', 'couple', 'to', 'win', 'the', 'nobel', 'prize', 'and', 'launching', 'the', 'curie', 'family', 'legacy', 'of', 'nobel', 'prizes']\n"
     ]
    }
   ],
   "source": [
    "print(simple_preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5b31e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [from, lerxst, wam, umd, edu, where, my, thing...\n",
       "1    [from, guykuo, carson, washington, edu, guy, k...\n",
       "2    [from, twillis, ec, ecn, purdue, edu, thomas, ...\n",
       "3    [from, jgreen, amber, joe, green, subject, re,...\n",
       "4    [from, jcm, head, cfa, harvard, edu, jonathan,...\n",
       "Name: tokens_simple, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens_simple'] = df['text'].apply(simple_preprocess)\n",
    "df['tokens_simple'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5140acc9",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Count and frequencies\n",
    "#### Tokens\n",
    "\n",
    "- Token $=$ the most basic unit of representation in a text\n",
    "\n",
    "- A token is a sequence of characters that we want to treat as a group\n",
    "  - Usually, a word\n",
    "  - But could be a phrase, a number, a punctuation mark, etc.\n",
    "  - $N-$ grams: sequences of $N$ tokens\n",
    "    - Moving window, for instance “hello world, i am online now” becomes “(hello world),(world i), (i am), (am online), (online now)”\n",
    "    - Learn a vocabulary of phrases and tokenize those: “Liège University → liege_university”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cfbde4c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marie', 'curie', 'was', 'the', 'first', 'woman', 'to', 'win', 'a', 'nobel', 'prize', 'the', 'first', 'person', 'to', 'win', 'a', 'nobel', 'prize', 'twice', 'and', 'the', 'only', 'person', 'to', 'win', 'a', 'nobel', 'prize', 'in', '2', 'scientific', 'fields', 'her', 'husband', 'pierre', 'curie', 'was', 'a', 'cowinner', 'of', 'her', 'first', 'nobel', 'prize', 'making', 'them', 'the', 'first', 'married', 'couple', 'to', 'win', 'the', 'nobel', 'prize', 'and', 'launching', 'the', 'curie', 'family', 'legacy', 'of', '5', 'nobel', 'prizes']\n"
     ]
    }
   ],
   "source": [
    "tokens = text_nopunc.split() # splits a string on white space\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf9b92e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Removing numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "039b83a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marie', 'curie', 'was', 'the', 'first', 'woman', 'to', 'win', 'a', 'nobel', 'prize', 'the', 'first', 'person', 'to', 'win', 'a', 'nobel', 'prize', 'twice', 'and', 'the', 'only', 'person', 'to', 'win', 'a', 'nobel', 'prize', 'in', 'scientific', 'fields', 'her', 'husband', 'pierre', 'curie', 'was', 'a', 'cowinner', 'of', 'her', 'first', 'nobel', 'prize', 'making', 'them', 'the', 'first', 'married', 'couple', 'to', 'win', 'the', 'nobel', 'prize', 'and', 'launching', 'the', 'curie', 'family', 'legacy', 'of', 'nobel', 'prizes']\n"
     ]
    }
   ],
   "source": [
    "# remove numbers (keep if not a digit)\n",
    "no_numbers = [t for t in tokens if not t.isdigit()]\n",
    "print(no_numbers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4771f036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marie', 'curie', 'was', 'the', 'first', 'woman', 'to', 'win', 'a', 'nobel', 'prize', 'the', 'first', 'person', 'to', 'win', 'a', 'nobel', 'prize', 'twice', 'and', 'the', 'only', 'person', 'to', 'win', 'a', 'nobel', 'prize', 'in', '#', 'scientific', 'fields', 'her', 'husband', 'pierre', 'curie', 'was', 'a', 'cowinner', 'of', 'her', 'first', 'nobel', 'prize', 'making', 'them', 'the', 'first', 'married', 'couple', 'to', 'win', 'the', 'nobel', 'prize', 'and', 'launching', 'the', 'curie', 'family', 'legacy', 'of', '#', 'nobel', 'prizes']\n"
     ]
    }
   ],
   "source": [
    "# keep if not a digit, else replace with \"#\"\n",
    "norm_numbers = [t if not t.isdigit() else '#'\n",
    "                for t in tokens ]\n",
    "print(norm_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae46fda6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e70d0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marie', 'curie', 'first', 'woman', 'win', 'nobel', 'prize', 'first', 'person', 'win', 'nobel', 'prize', 'twice', 'person', 'win', 'nobel', 'prize', '#', 'scientific', 'fields', 'husband', 'pierre', 'curie', 'cowinner', 'first', 'nobel', 'prize', 'making', 'first', 'married', 'couple', 'win', 'nobel', 'prize', 'launching', 'curie', 'family', 'legacy', '#', 'nobel', 'prizes']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords # Stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "# keep if not a stopword\n",
    "nostop = [t for t in norm_numbers if t not in stoplist]\n",
    "print(nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ece50f87",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 6),\n",
       " ('nobel', 6),\n",
       " ('prize', 5),\n",
       " ('first', 4),\n",
       " ('to', 4),\n",
       " ('win', 4),\n",
       " ('a', 4),\n",
       " ('curie', 3),\n",
       " ('was', 2),\n",
       " ('person', 2)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counter is a quick pure-python solution.\n",
    "from collections import Counter\n",
    "freqs = Counter(tokens)\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578caf9b",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3. N-grams\n",
    "\n",
    "- N-grams are phrases, sequences of words up to length N.\n",
    "  - Bigrams, trigrams, quadgrams, etc\n",
    "<img src=\"images/n-grams.png\"  style=\"height: 350px; position:relative;     margin-left: auto;margin-right: auto;display: block\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2495a41",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### N-grams and high dimensionality\n",
    "\n",
    "- N-grams will blow up the feature space:\n",
    "  - Thus, filtering out uninformative N-grams is necessary\n",
    "- The right number of features depends on the application\n",
    "  - I have gotten good performance with e.g., 2000 features\n",
    "- For supervised learning tasks, a decent “rule of thumb” is to build a vocabulary of 60K, then use feature selection to get down to 10K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e698395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 41),\n",
       " (('subject:', 're:'), 37),\n",
       " (('in', 'the'), 33),\n",
       " (('to', 'the'), 27),\n",
       " (('i', 'am'), 21),\n",
       " (('i', 'have'), 21),\n",
       " (('to', 'be'), 19),\n",
       " (('on', 'the'), 18)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# get n-gram counts for 10 documents\n",
    "grams = []\n",
    "for i, row in df.iterrows():\n",
    "    tokens = row['text'].lower().split() # get tokens\n",
    "    for n in range(2,4):\n",
    "        grams += list(ngrams(tokens,n)) # get bigrams, trigrams, and quadgrams\n",
    "    if i > 50:\n",
    "        break\n",
    "Counter(grams).most_common()[:8]  # most frequent n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0614df",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4. Parts of speech\n",
    "\n",
    "- Parts of speech (POS) tags provide useful word categories corresponding to their functions in sentences\n",
    "  - Eight main parts of speech: verb (VB), noun (NN), pronoun (PR), adjective (JJ), adverb (RB), determinant (DT), preposition (IN), conjunction (CC).\n",
    "\n",
    "- POS vary in their informativeness for various functions\n",
    "  - For categorizing topics, nouns are usually most important\n",
    "  - For sentiment, adjectives are usually most important\n",
    "- One can count POS tags as features – e.g., using more adjectives, or using more passive verbs\n",
    "- POS n-gam frequencies (e.g. NN, NV, VN, ...), like function words, are good stylistic features for authorship detection\n",
    "  - Not biased by topics/content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1fb849",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Install spaCy and download the model\n",
    "```bash\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25aa04f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5e550",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Parts of speech tagging with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5cde955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = df.sample(10)\n",
    "dfs['doc'] = dfs['text'].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d9b1528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: From, POS: ADP\n",
      "Token: :, POS: PUNCT\n",
      "Token: wcd82671@uxa.cso.uiuc.edu, POS: PROPN\n",
      "Token: (, POS: PUNCT\n",
      "Token: daniel, POS: PROPN\n",
      "Token: warren, POS: PROPN\n",
      "Token: c, POS: PROPN\n",
      "Token: ), POS: PUNCT\n",
      "Token: \n",
      ", POS: SPACE\n",
      "Token: Subject, POS: NOUN\n"
     ]
    }
   ],
   "source": [
    "doc = dfs['doc'].iloc[0]\n",
    "\n",
    "for token in doc[:10]:\n",
    "    print(f\"Token: {token.text}, POS: {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3abfc0",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5. Named Entity Recognition\n",
    "\n",
    "- Refers to the task of identifying named entities such as \"December 1903\" and Pierre Curie”, which can be used as tokens\n",
    "\n",
    "- Detecting the type requires a trained model (e.g. spaCy)\n",
    "  - Common types: persons, organizations, locations, dates, etc.\n",
    "\n",
    "<img src=\"images/ner-illustration.png\"  style=\"height: 140px; position:relative;     margin-left: auto;margin-right: auto;display: block\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d75a6c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[irwin@cmptrc.lonestar.org,\n",
       " (Irwin Arnstein,\n",
       " Subject,\n",
       " Recommendation,\n",
       " Duc\n",
       " Summary,\n",
       " What,\n",
       " it,\n",
       " Distribution,\n",
       " usa,\n",
       " Sat,\n",
       " May 1993 05:00:00 GMT\n",
       " Organization,\n",
       " CompuTrac Inc.,\n",
       " Richardson TX\n",
       " Keywords,\n",
       " Ducati,\n",
       " GTS,\n",
       " I,\n",
       " a line,\n",
       " a Ducati 900GTS 1978 model,\n",
       " the clock,\n",
       " paint]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spacy NER noun chunks\n",
    "chunks = list(nlp(df['text'].iloc[10]).noun_chunks)\n",
    "chunks[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa291db9",
   "metadata": {},
   "source": [
    "### Bag-of-words representation\n",
    "\n",
    "- The most common way to represent text data $D$ (ie a corpus) is as a matrix $X$ of token counts\n",
    "  - Each row is a document, each column is a token\n",
    "  - The value in each cell is the count of that token in that document\n",
    "\n",
    "- More generally, “bag-of-terms” representation refers to counts over any informative features – e.g. N-grams, syntax features, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7760bdcc",
   "metadata": {},
   "source": [
    "#### scikit-learn's CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064ff5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 526707 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(min_df=0.001, # at min 0.1% of docs\n",
    "                        max_df=.8, # drop if shows up ih more than 80%\n",
    "                        max_features=1000,\n",
    "                        stop_words='english',\n",
    "                        ngram_range=(1,3)) # words, bigrams, and trigrams\n",
    "X = vec.fit_transform(df['text'])\n",
    "\n",
    "# save the vectors\n",
    "# pd.to_pickle(X,'X.pkl')\n",
    "\n",
    "# save the vectorizer\n",
    "# (so you can transform other documents, also for the vocab)\n",
    "#pd.to_pickle(vec, 'vec-3grams-1.pkl')\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5471035",
   "metadata": {},
   "source": [
    "#### Counts and frequencies\n",
    "\n",
    "- Document counts: number of documents where a token appears\n",
    "- Term counts: number of total appearances of a token in corpus\n",
    "- Term frequency:\n",
    "  $$\\text{Term Frequency of w in document d} = \\frac{\\text{Count of w in document d}}{\\text{Total tokens in document d}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fec812b",
   "metadata": {},
   "source": [
    "#### Building a vocabulary\n",
    "\n",
    "- An important featurization step is to build a vocabulary of words:\n",
    "  - Compute (document) frequencies for all words\n",
    "  - Inspect low-frequency words and determine a minimum document threshold\n",
    "    - For instance: 10 documents, or .25% of documents\n",
    "- Can also impose more complex thresholds, e.g.:\n",
    "  - Appears twice in at least 20 documents\n",
    "  - Appears in at least 3 documents in at least 5 years\n",
    "- Assign numerical identifiers to tokens to increase speed and reduce disk usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7297161",
   "metadata": {},
   "source": [
    "#### TF-IDF (Term Frequency-Inverse Document Frequency) weighting\n",
    "\n",
    "- TF/IDF: “term-frequency / inverse-document-frequency”\n",
    "- The formula for word $w$ in document $k$:\n",
    "$$\\text{TF-IDF}(w, k) = \\frac{\\text{Count of w in k}}{\\text{Total word of k}} \\times \\log\\left(\\frac{number of documents in D}{\\text{number of documents where w appears}}\\right)$$\n",
    "\n",
    "- The formula up-weights relatively rare words that do not appear in all documents\n",
    "  - These words are probably more distinctive of topics or differences between documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e8cbaf",
   "metadata": {},
   "source": [
    "#### scikit-learn’s TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdad06ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 521387 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tf-idf vectorizer up-weights rare/distinctive words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df=0.001,\n",
    "                        max_df=0.9,\n",
    "                        max_features=1000,\n",
    "                        stop_words='english',\n",
    "                        use_idf=True, # the new piece\n",
    "                        ngram_range=(1,2))\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(df['text'])\n",
    "#pd.to_pickle(X_tfidf,'X_tfidf.pkl')\n",
    "X_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d3750e",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- .slide:  id=\"distance\" class: left, inverse -->\n",
    "## Measures of document distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f35484",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In economics, we often want to compare documents (broadly defined) to one another\n",
    "\n",
    "  - For instance, how close is a political speech to the party leader?\n",
    "\n",
    "- Now, we focus on methods designed to measure document distance/proximity\n",
    "\n",
    "- Almost all content from this lecture can be framed as measuring document distance in some way\n",
    "  - all \"text representations\" can be used to measure document distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffed650",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Document-term matrix\n",
    "\n",
    "- The document-term matrix $\\mathbf{X}$ is a matrix where \n",
    "  - Each row $d$ corresponds to a **document** \n",
    "  - Each column corresponds to a **term** (word or token).\n",
    "\n",
    "- A matrix entry $\\mathbf{X}_{[d,w]}$ quantifies the strength of association between a document $d$ and a word $w$, \n",
    "  - generally its count or frequency\n",
    "\n",
    "| Document | Word1 | Word2 | Word3 | Word4 |\n",
    "|----------|-------|-------|-------|-------|\n",
    "| Doc1     |   2   |   1   |   0   |   1   |\n",
    "| Doc2     |   0   |   3   |   1   |   0   |\n",
    "| Doc3     |   1   |   0   |   4   |   2   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c3d24",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Each row $\\mathbf{X}_{[d,:]}$ is a document vector of the distribution over terms\n",
    "  - These vectors have a spatial interpretation \n",
    "    - $\\rightarrow$ geometric distances between document vectors reflect semantic distances between documents in terms of shared terms\n",
    "\n",
    "- Each column $\\mathbf{X}_{[:,w]}$ is term vector of a distribution over documents\n",
    "  - *also* have a spatial interpretation \n",
    "    - $\\rightarrow$ geometric distances between term vectors reflect semantic distances between words in terms of showing up in the same documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd3fda7",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cosine similarity\n",
    "- Each document is \n",
    "  - a vector $\\mathbf{x}_{d}$ e.g. token counts or TF-IDF frequencies\n",
    "  - Similar documents have similar vectors\n",
    "\n",
    "- Can measure similarity between documents $i$ and $j$ by the cosine of the angle between $\\mathbf{x_i}$ and $\\mathbf{x_j}$\n",
    "  - With perfectly collinear documents (that is, $\\mathbf{x_i} = \\alpha \\mathbf{x_j}$ , $\\alpha > 0$), $\\cos(0) = 1$\n",
    "  - For orthogonal documents (no words in common), $\\cos(\\pi/2) = 0$\n",
    "\n",
    "- Cosine similarity is computable as the normalized dot product of the two vectors:\n",
    "$$\\text{cosine similarity}(\\mathbf{x_i}, \\mathbf{x_j}) = \\frac{\\mathbf{x_i} \\cdot \\mathbf{x_j}}{||\\mathbf{x_i}|| \\cdot ||\\mathbf{x_j}||}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03377e51",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute pair-wise similarities between all documents in corpus\"\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sim = cosine_similarity(X[:100])\n",
    "sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f172d074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.20384233, 0.15095711, 0.19219753],\n",
       "       [0.20384233, 1.        , 0.12569587, 0.1608558 ],\n",
       "       [0.15095711, 0.12569587, 1.        , 0.16531366],\n",
       "       [0.19219753, 0.1608558 , 0.16531366, 1.        ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim[:4,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9cf45fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.05129256, 0.08901433, 0.06064389],\n",
       "       [0.05129256, 1.        , 0.07497709, 0.03570566],\n",
       "       [0.08901433, 0.07497709, 1.        , 0.09077347],\n",
       "       [0.06064389, 0.03570566, 0.09077347, 1.        ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Similarity\n",
    "tsim = cosine_similarity(X_tfidf[:100])\n",
    "tsim[:4,:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7bcbb3",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Cosine similarity\n",
    "\n",
    "- For a corpus with $n$ rows, the pairwise similarities give $n \\times (n − 1)$ similarity scores\n",
    "\n",
    "- $TF-IDF$ down-weights terms that appear in many documents\n",
    "  - Usually gives better results\n",
    "- Alternative distance metrics:\n",
    "  - dot product and Euclidean distance are too sensitive to document length\n",
    "  - Jensen-Shannon divergence\n",
    "  - Jaccard distance\n",
    "  - Etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b0152e",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c704d9d9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### k-means clustering\n",
    "- Method to partition the observations (documents) into $k$ clusters $ S_1, S_2, \\ldots, S_k $:\n",
    "    - Each cluster is represented by its centroid $ \\mu_i $\n",
    "    - Each document is assigned to the cluster with the closest centroid\n",
    "    - $k$ (number of clusters) is the only hyperparameter\n",
    "\n",
    "- **Algorithm**:\n",
    "    - Initialize cluster centroids randomly\n",
    "    - Shift them around to minimize the sum of the within-cluster squared distance (features should be standardized)\n",
    "\n",
    "        $$\\arg\\min_{S_1, ..., S_k}\\sum_{i=1}^k\\sum_{x \\in S_i}||x - \\mu_i||^2$$\n",
    "\n",
    "    - Repeat until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea822ba1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/k-means.png\" alt=\"NLP Bias\" style=\"width: 200%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f9014",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Other clustering algorithms\n",
    "\n",
    "- “k-medoid” clustering use L1 distance rather than Euclidean distance\n",
    "  - Produces each cluster’s “medoid” (median vector) instead of “centroid” (mean vector)\n",
    "  - Less sensitive to outliers\n",
    "  - The medoid can be used as a representative data point\n",
    "\n",
    "- DBSCAN defines clusters as continuous regions of high density\n",
    "  - Detects and excludes outliers automatically\n",
    "\n",
    "- Agglomerative (hierarchical) clustering makes nested clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4732f39",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Final Notes on $\\mathbf{X}$\n",
    "\n",
    "- Each row $\\mathbf{X}_{[d,:]}$ is a document vector of the distribution over terms\n",
    "- Each column $\\mathbf{X}_{[:,w]}$ is term vector of a distribution over documents\n",
    "\n",
    "- The same methods we used on the rows can be used on the columns:\n",
    "  - Apply cosine similarity to the columns to compare words (rather than compare documents)\n",
    "  - Apply $k-$means clustering to the columns to get clusters of similar words (rather than clusters of documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c94f191",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- .slide:  id=\"topic\" class: left, inverse -->\n",
    "## Topic models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c132c9",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Topic models\n",
    "\n",
    "- Summarize unstructured text\n",
    "\n",
    "- Use words within the document to infer the subject\n",
    "- Interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6942fba",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A stylized example\n",
    "- A corpus of documents\n",
    "  - Doc 1: guns zombies biohazard win lose...\n",
    "  - Doc 2: player lose score survival...\n",
    "  - Doc 3: zombies survival congress...\n",
    "  - Doc 4: ...\n",
    "  - Doc 100000: congress welfare constitution guns...\n",
    "\n",
    "- What are the topics in these documents?\n",
    "  - <p style=\"color: red;\">Zombies: guns, zombies, biohazard, survival</p>\n",
    "  - <p style=\"color: blue;\">Sports: player, win, score, lose</p>\n",
    "  - <p style=\"color: green;\">Politics: welfare, congress, constitution, guns</p>\n",
    "\n",
    "How does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564073a7",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Topic models\n",
    "\n",
    "- Topics models infer *latent* topics in the corpus:\n",
    "  - Documents as distributions over topics\n",
    "  - Topics as distributions over words\n",
    "\n",
    "- **Main assumption**: The number of topics $K$ is a hyperparameter.\n",
    "- In the original models, formally, $\\mathbf{W}$ is decomposed into two matrices:\n",
    "$$\\mathbf{W} = \\mathbf{\\Theta}\\times \\mathbf{B}^T$$\n",
    "where $\\mathbf{W}\\in D\\times V$ is the document-term matrix, $\\mathbf{\\Theta}\\in D\\times K$ is the document-topic matrix, and $\\mathbf{B}\\in V\\times K$ is the topic-term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d618392",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "- The most popular topic model\n",
    "\n",
    "- Each document is a mixture of topics\n",
    "- Each topic is a mixture of words\n",
    "- The model is generative:\n",
    "  - For each document, draw a distribution over topics\n",
    "  - For each word in the document, draw a topic from the distribution over topics\n",
    "  - For each word, draw a word from the distribution over words for the topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa097e0c",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using an LDA model\n",
    "Once trained, one can easily get topic proportions for a corpus\n",
    "- For any document – doesn’t have to be in training corpus\n",
    "\n",
    "- The main topic is the highest-probability topic\n",
    "- Documents with the highest share in a topic work as representative documents for the topic\n",
    "- One can use the topic proportions as variables in a social science analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69913a7e",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Application \n",
    "#### Analyzing business news ...\n",
    "\n",
    "<img src=\"images/bybee-topics.png\"  style=\"height: 350px; position:relative;     margin-left: auto;margin-right: auto;display: block\" >\n",
    "\n",
    "<div style=\"font-size: smaller;\">\n",
    "Source: Bybee, L., Kelly, B., Manela, A., & Xiu, D. (2024). Business News and Business Cycles. Journal of Finance, 79(4), 3105-3147. \n",
    "<a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/jofi.13377\">link</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0e2536",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Detail of two-dimensional embedding for article whose dominant topic is “Federal Reserve.” Articles within this set are colored according to their second largest topic proportion.\n",
    "\n",
    "> We propose an approach to measuring the state of the economy via textual analysis of business news. From the full text of $800,000$ Wall Street Journal articles for 1984 to 2017, we estimate a topic model that summarizes business news into interpretable topical themes and quantifies the proportion of news attention allocated to each theme over time. News attention closely tracks a wide range of economic activities and can forecast aggregate stock market returns. A text-augmented vector autoregression demonstrates the large incremental role of news text in forecasting macroeconomic dynamics. We retrieve the narratives that underlie these improvements in market and business cycle forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db09e10d",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### ... to predict macroeconomic variables.\n",
    "<img src=\"images/bybee-predict.png\"  style=\"height: 350px; position:relative;     margin-left: auto;margin-right: auto;display: block\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5dc31e",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- .slide:  id=\"embeddings\" class: left, inverse -->\n",
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115df172",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Where we are\n",
    "#### Different ways to represent text data\n",
    "- Dictionary methods\n",
    "  - document is represented as a count over the lexicon\n",
    "- N-grams (tokenisation)\n",
    "  - document is a count over a vocabulary of phrases\n",
    "- Topic models\n",
    "  - document is a vector of shares over topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802486ae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Text classifiers\n",
    "- produce $\\hat y_i=f(\\mathbf{x_i}, \\hat\\theta)$ a vector of predicted probabilities across classes for each document $i$\n",
    "  - $y_i$ is a vector of class probabilities ie. a compressed representation of the text features $\\mathbf{x_i}$\n",
    "  - $\\mathbf{x_i}$: matrix of features is itslef a compressed representation of the document\n",
    "  - the learned parameters $\\hat\\theta$ can be understood as a compressed representation of the data\n",
    "  - $\\hat \\theta$ contains information about the training corpus, the text features, and the oucomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0bae47",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Limitations of bag-of-words representations\n",
    "- Until now, $\\mathbf{x_i}$ has been a “bag-of-words” representation.\n",
    "- Bag-of-words representations disregard syntax\n",
    "  - “The cat ate the mousse.” versus The mousse ate the cat.”\n",
    "  - $\\rightarrow$ These two sentences have the same bag-of-words representation\n",
    "\n",
    "- Bag-of-words representations disregard semantic proximity between words\n",
    "  - “hi” and “hello” are completely distinct features for predicting whether a message is greeting somebody\n",
    "  - “economics” and “sociology” are distinct features for predicting whether a message is about the social sciences\n",
    "\n",
    "##### Can we estimate text features that capture semantic proximity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0a2478",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Word embeddings\n",
    "\n",
    "- Fancy word, old concept\n",
    "- Vector representation of a word (we have already seen count-vectorizer, tf-idf)\n",
    "- What we mean by word embedding is that we are embedding a categorical entity into a vector space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc991d3",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## An example to build some intuition\n",
    "\n",
    "#### Can you complete this text snippet?\n",
    "<img src=\"images/embedding-1.png\"  style=\"width: 550px; position:relative;     margin-left: auto;margin-right: auto;display: block\" >\n",
    "<div style=\"font-size: smaller;\">\n",
    "Source: Patrick Harrison, S&P Global Market Intelligence\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce405531",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## An example to build some intuition\n",
    "\n",
    "#### Can you complete this text snippet?\n",
    "<img src=\"images/embedding-2.png\"  style=\"width: 550px; position:relative;     margin-left: auto;margin-right: auto;display: block\" >\n",
    "<div style=\"font-size: smaller;\">\n",
    "Source: Patrick Harrison, S&P Global Market Intelligence\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3059f7b9",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Language in context (and vice-versa)\n",
    "- Neighboring words provide us with additional information to interpret a word’s meaning\n",
    "\n",
    "- In other words, word co-occurrences capture context\n",
    "- This information is useful for machine learning applications\n",
    "  - For example, document classification, machine translation, syntax prediction, machine comprehension, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffee92e3",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Best known word embeddings model: Word2Vec\n",
    "- Word2Vec reformulates learning word co-occurrences as two prediction tasks:\n",
    "  - **Continuous Bag of Words** (CBOW): Given its context words, predict a focus word\n",
    "  - **Skipgram**: Given a focus word, predict all its context words\n",
    "\n",
    "- In both cases, the model results in a low-dimensional, dense vector space representation of $C$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5153fa",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distance between texts\n",
    "\n",
    "- With embeddings, we can use linear algebra to understand relationships between words\n",
    "- In particular, words that are geometrically close to each other are similar\n",
    "- The standard metric for comparing vectors is cosine similarity\n",
    "\n",
    "$$\\text{cosine similarity}(x_i, x_j) = \\frac{x_i \\cdot x_j}{||x_i|| \\cdot ||x_j||}$$\n",
    "\n",
    "- When vectors are normalized, cosine similarity is:\n",
    "  - Simply the dot product of both vectors\n",
    "  - Proportional to the Euclidean distance (so you can use it, too)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7ffe59",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distance between texts\n",
    "\n",
    "<img src=\"images/embedding-eg.png\"  style=\"width: 550px; position:relative;     margin-left: auto;margin-right: auto;display: block\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272bacc7",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualizing embeddings\n",
    "\n",
    "- One can also visualize the resulting embedding space by projecting it on a two-dimensional space\n",
    "\n",
    "- Three commonly used techniques are:\n",
    "  - Principal Component Analysis (PCA)\n",
    "  - t-distributed stochastic neighbor embedding (t-SNE)\n",
    "  - Uniform Manifold Approximation and Projection (UMAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72143820",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Basic arithmetic often carries meaning\n",
    "- Word2vec algebra can depict conceptual, analogical relationships between words.\n",
    "\n",
    "  - eg. $\\overrightarrow{\\text{king}} - \\overrightarrow{ \\text{man}} + \\overrightarrow{wo⃗man} ≈ \\overrightarrow{qu⃗een}$\n",
    "\n",
    "<img src=\"images/embedding-eg2.png\"  style=\"width: 750px; position:relative;     margin-left: auto;margin-right: auto;display: block\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb2a387",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Some refinements\n",
    "- The main assumption behind word2vec is that context words are exchangeable\n",
    "\n",
    "- In other words, the ordering of words is not accounted for\n",
    "- Recent models relax this assumption; they are called transformers...\n",
    "- .. and consistently outperform previous language models in various tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cebcce",
   "metadata": {
    "slide_type": "subslide",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pros and cons of embeddings\n",
    "\n",
    "- Pros:\n",
    "  - Many pre-trained models for different languages are freely available online\n",
    "  - Many packages to train models from scratch or fine-tune existing models to a specific corpus\n",
    "  - Often, they provide sizable gains in prediction accuracy\n",
    "\n",
    "- Cons:\n",
    "  - Clear loss of interpretability relative to bag-of-words\n",
    "  - Neighbouring words are not the only forms of context\n",
    "  - Often critiqued as “stochastic parrots” (Bender et al., 2021)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsg-spring2025b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
